{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dissertation_N-BaIoT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Package Installations**"
      ],
      "metadata": {
        "id": "TlPNinbl7JAM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUpffPkc6uZG"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-gpu==1.15.5\n",
        "!pip install git+https://github.com/openai/baselines.git@ea25b9e8b234e6ee1bca43083f8f3cf974143998"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Library Imports**"
      ],
      "metadata": {
        "id": "nMQZeYGQ7Xqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import gym\n",
        "import random\n",
        "import functools\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from baselines.ppo2 import ppo2\n",
        "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
        "\n",
        "from baselines import deepq\n",
        "from baselines import bench\n",
        "from baselines import logger\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.semi_supervised import LabelPropagation\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.cluster import KMeans\n",
        "from baselines.common.tf_util import make_session\n",
        "\n",
        "from baselines.common import set_global_seeds, explained_variance\n",
        "from baselines.common import tf_util\n",
        "from baselines.common.policies import build_policy\n",
        "from baselines.common.tf_util import get_session, save_variables, load_variables\n",
        "\n",
        "from baselines.a2c.utils import Scheduler, find_trainable_variables\n",
        "from baselines.a2c.runner import Runner\n",
        "from baselines.ppo2.ppo2 import safemean\n",
        "from collections import deque\n",
        "from baselines.acktr import kfac\n",
        "\n",
        "from tensorflow import losses\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D"
      ],
      "metadata": {
        "id": "5DAoOuXD7Y_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Preprocessing: No Attack**"
      ],
      "metadata": {
        "id": "tWYFPBOX9Cwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concatenated_df = pd.read_csv('n-baiot_combined.csv')\n",
        "concatenated_df.columns = concatenated_df.columns.str.strip()\n",
        "print(\"original length of df:\", len(concatenated_df))\n",
        "concatenated_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "concatenated_df.dropna(inplace=True)\n",
        "print(\"after droping null values, the length of df:\", len(concatenated_df))\n",
        "normal_class = 'benign'\n",
        "output_column = 'Labels_binary'\n",
        "concatenated_df[output_column] = concatenated_df['type'].apply(lambda x: 0 if x==normal_class else 1)\n",
        "concatenated_df.head()\n",
        "X = concatenated_df.drop(columns=[output_column]).copy()\n",
        "Y = concatenated_df[output_column]\n",
        "class_le = LabelEncoder()\n",
        "Y = class_le.fit_transform(Y)\n",
        "multiclass_le = LabelEncoder()\n",
        "X['type'] = multiclass_le.fit_transform(X['type'])\n",
        "remove = []\n",
        "cat_cols = []\n",
        "for c in X.columns:\n",
        "    \n",
        "# Columns with unique value\n",
        "    if X[c].nunique() == 1:\n",
        "        remove.append(c)\n",
        "        \n",
        "# Categorical columns\n",
        "    elif X[c].nunique()<10:\n",
        "        cat_cols.append(c)\n",
        "    print(c, X[c].nunique())\n",
        "\n",
        "cat_cols.append('type')\n",
        "# Remove columns with unique value\n",
        "X.drop(columns=remove, inplace=True)\n",
        "X_discrete = X[cat_cols]\n",
        "number_of_discrete = len(cat_cols)\n",
        "X.drop(columns=cat_cols, inplace=True)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "num_of_continuous = len(X_scaled.columns)\n",
        "\n",
        "# combine categorical and scaled dataframes\n",
        "X_merge=pd.concat([X_scaled, X_discrete], axis=1, join='inner')\n",
        "\n",
        "indices=X_scaled.index.difference(X_merge.index)\n",
        "out = [val for i, val in enumerate(Y) if all(i != indices)]\n",
        "Y_merge=np.array(out)\n",
        "\n",
        "\n",
        "seed = 10\n",
        "test_size = 0.2\n",
        "X_train, X_test, Y_train, Y_test= train_test_split(X_merge, Y_merge, test_size= test_size, random_state=seed)\n",
        "test_size = 0.25\n",
        "X_train, X_val, Y_train, Y_val= train_test_split(X_train, Y_train, test_size= test_size, random_state=seed) \n",
        "    \n",
        "Y_train_multi = X_train['type']\n",
        "Y_test_multi = X_test['type']\n",
        "X_train.drop(columns=['type'], inplace=True)\n",
        "X_val.drop(columns=['type'], inplace=True)\n",
        "X_test.drop(columns=['type'], inplace=True)"
      ],
      "metadata": {
        "id": "Zr8vZ6iG9Dxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Features & Plotting**"
      ],
      "metadata": {
        "id": "_Jv9GY1WCJkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count=concatenated_df['type'].value_counts()\n",
        "values=[]\n",
        "for i in range(concatenated_df['type'].nunique()):\n",
        "    values.append(count[i])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,2,2])\n",
        "ax.bar(concatenated_df['type'].unique(),values)\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# assign bars to a variable so attributes can be accessed\n",
        "x=concatenated_df['type'].unique()\n",
        "bars = plt.bar(x, height=values, width=0.8)\n",
        "\n",
        "# access bar attributes to place text in appropriate location\n",
        "for bar in bars:\n",
        "    y = bar.get_height()\n",
        "    plt.text(bar.get_x(), y, y)\n",
        "plt.show()\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "SztBQikICMT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Neural Network Implementation and Evaluation: No Attack**"
      ],
      "metadata": {
        "id": "R03eyUoJ9QlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model / data parameters\n",
        "num_classes = 1\n",
        "input_shape = X_train.shape[1]\n",
        "\n",
        "def DNN_Model(X, Y, batch_size=32, epochs=2):\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            keras.Input(shape=input_shape),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(num_classes, activation=\"sigmoid\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    start_time = time.time()\n",
        "    model.fit(X, Y, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "    end_time = time.time()\n",
        "\n",
        "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    p_pred = model.predict(X_test)\n",
        "    p_pred = p_pred.flatten()\n",
        "    y_pred = np.where(p_pred > 0.5, 1, 0)\n",
        "    print(\"Test loss:\", score[0])\n",
        "    print(\"Test accuracy:\", score[1])\n",
        "    print(confusion_matrix(Y_test, y_pred))\n",
        "    print(\"Training Time:\", end_time - start_time)\n",
        "\n",
        "DNN_Model(X_train,Y_train)"
      ],
      "metadata": {
        "id": "FYtBFm_m9Oe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reinforcement Learning Environment Creation**"
      ],
      "metadata": {
        "id": "LY2OW3_X90iI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_reshape=X_train.values.reshape(X_train.shape[0],X_train.shape[1],1)\n",
        "X_test_reshape=X_test.values.reshape(X_test.shape[0],X_test.shape[1],1)\n",
        "\n",
        "class IoTEnv(gym.Env):\n",
        "    def __init__(self, data_per_episode=1, dataset=(X_train_reshape, Y_train), random=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(concatenated_df['type'].nunique())\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1,\n",
        "                                                shape=(X_train.shape[1], 1),\n",
        "                                                dtype=np.float32)\n",
        "\n",
        "        self.data_per_episode = data_per_episode\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.x, self.y = dataset #features, labels\n",
        "        self.random = random\n",
        "        self.dataset_idx = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        done = False\n",
        "        reward = int(action == self.expected_action)\n",
        "\n",
        "        obs = self._next_obs()\n",
        "\n",
        "        self.step_count += 1\n",
        "        if self.step_count >= self.data_per_episode:\n",
        "            done = True\n",
        "\n",
        "        return obs, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.step_count = 0\n",
        "\n",
        "        obs = self._next_obs()\n",
        "        return obs\n",
        "\n",
        "    def _next_obs(self):\n",
        "        if self.random:\n",
        "            next_obs_idx = random.randint(0, len(self.x) - 1)\n",
        "            self.expected_action = int(self.y[next_obs_idx])\n",
        "            obs = self.x[next_obs_idx]\n",
        "\n",
        "        else:\n",
        "            obs = self.x[self.dataset_idx]\n",
        "            self.expected_action = int(self.y[self.dataset_idx])\n",
        "\n",
        "            self.dataset_idx += 1\n",
        "            if self.dataset_idx >= len(self.x):\n",
        "                raise StopIteration()\n",
        "\n",
        "        return obs"
      ],
      "metadata": {
        "id": "bTWbvKl-913Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Q-Network Implementation**"
      ],
      "metadata": {
        "id": "86YztRAxC3xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IoT_dqn(X,Y):\n",
        "    env = IoTEnv(data_per_episode=1, dataset=(X, Y))\n",
        "    env = bench.Monitor(env, logger.get_dir())\n",
        "\n",
        "    model = deepq.learn(\n",
        "        env,\n",
        "        \"mlp\",\n",
        "        num_layers=1,\n",
        "        num_hidden=64,\n",
        "        activation=tf.nn.relu,\n",
        "        hiddens=[32],\n",
        "        dueling=True,\n",
        "        lr=1e-3,\n",
        "        total_timesteps=int(1.0e5),\n",
        "        buffer_size=10000,\n",
        "        exploration_fraction=0.001,\n",
        "        exploration_final_eps=0.0001,\n",
        "        train_freq=4,\n",
        "        learning_starts=10000,\n",
        "        target_network_update_freq=1000,\n",
        "    )\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return model\n",
        "\n",
        "start_time = time.time()\n",
        "dqn_model1 = IoT_dqn(X_train_reshape,Y_train)\n",
        "print(\"DQN Training Time:\", time.time() - start_time)"
      ],
      "metadata": {
        "id": "hcFy2xwgC6cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Q-Network Evaluation: No Attack**"
      ],
      "metadata": {
        "id": "wLgD_d8uD5Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IoT_dqn_eval(dqn_model):\n",
        "    attempts, correct, tp, fp, fn, tn, i = 0,0,0,0,0,0,0\n",
        "\n",
        "    env = IoTEnv(data_per_episode=1, dataset=(X_test_reshape, Y_test), random=False)\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            obs, done = env.reset(), False\n",
        "            while not done:\n",
        "                obs, rew, done, _ = env.step(dqn_model(obs[None])[0])\n",
        "\n",
        "                attempts += 1\n",
        "                if rew > 0:\n",
        "                  correct += 1\n",
        "                if Y_test[i]==0 and rew == 1:\n",
        "                    tp += 1\n",
        "                elif Y_test[i]==0 and rew == 0:\n",
        "                    fp += 1\n",
        "                elif Y_test[i]==1 and rew == 0:\n",
        "                    fn += 1\n",
        "                elif Y_test[i]==1 and rew == 1:\n",
        "                    tn += 1\n",
        "                i+=1\n",
        "\n",
        "    except StopIteration:\n",
        "        print()\n",
        "        print('validation done...')\n",
        "        print('Accuracy: {0}%'.format((float(correct) / attempts) * 100))\n",
        "        print('False Positive Rate: {0}%'.format((float(fp) / (fp+tn)) * 100))\n",
        "        print('Confusion Matrix:')\n",
        "        print(tp,fp)\n",
        "        print(fn,tn)\n",
        "        print('Total no.of test values:',attempts)\n",
        "\n",
        "IoT_dqn_eval(dqn_model1)"
      ],
      "metadata": {
        "id": "z9FrQTBaEBt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proximal Policy Optimization (PPO) Implementation**"
      ],
      "metadata": {
        "id": "HgyPe7nyEupL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IoT_ppo(X,Y):\n",
        "    env = DummyVecEnv([lambda: bench.Monitor(IoTEnv(data_per_episode=1,dataset=(X, Y)), logger.get_dir())])\n",
        "\n",
        "    model = ppo2.learn(\n",
        "        env=env,\n",
        "        network='mlp',\n",
        "        num_layers=2,\n",
        "        num_hidden=64,\n",
        "        nsteps=16,\n",
        "        total_timesteps=int(1.0e5),\n",
        "        seed=int(time.time()))\n",
        "\n",
        "    return model\n",
        "\n",
        "start_time = time.time()\n",
        "ppo_model1 = IoT_ppo(X_train_reshape,Y_train)\n",
        "print(\"PPO Training Time:\", time.time() - start_time)"
      ],
      "metadata": {
        "id": "bJpTTpxPEw_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proximal Policy Optimization (PPO) Evaluation: No Attack**"
      ],
      "metadata": {
        "id": "Q72kTqKdFKa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IoT_ppo_eval(ppo_model):\n",
        "    attempts, correct, tp, fp, fn, tn, i = 0,0,0,0,0,0,0\n",
        "\n",
        "    env = DummyVecEnv([lambda: IoTEnv(data_per_episode=1, dataset=(X_test_reshape, Y_test), random=False)])\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            obs, done = env.reset(), [False]\n",
        "            while not done[0]:\n",
        "                obs, rew, done, _ = env.step(ppo_model.step(obs[None])[0])\n",
        "\n",
        "                attempts += 1\n",
        "                if rew > 0:\n",
        "                  correct += 1\n",
        "                if Y_test[i]==0 and rew == 1:\n",
        "                    tp += 1\n",
        "                elif Y_test[i]==0 and rew == 0:\n",
        "                    fp += 1\n",
        "                elif Y_test[i]==1 and rew == 0:\n",
        "                    fn += 1\n",
        "                elif Y_test[i]==1 and rew == 1:\n",
        "                    tn += 1\n",
        "                i+=1\n",
        "\n",
        "    except StopIteration:\n",
        "        print()\n",
        "        print('validation done...')\n",
        "        print('Accuracy: {0}%'.format((float(correct) / attempts) * 100))\n",
        "        print('False Positive Rate: {0}%'.format((float(fp) / (fp+tn)) * 100))\n",
        "        print('Confusion Matrix:')\n",
        "        print(tp,fp)\n",
        "        print(fn,tn)\n",
        "        print('Total no.of test values:',attempts)\n",
        "\n",
        "IoT_ppo_eval(ppo_model1)"
      ],
      "metadata": {
        "id": "4U-Qf7XNFLqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actor-Critic Model Creation**"
      ],
      "metadata": {
        "id": "uqI9J2XJGvfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model_a2c(object):\n",
        "     def __init__(self, policy, env, nsteps,\n",
        "                  ent_coef=0.01, vf_coef=0.5, max_grad_norm=0.5, lr=7e-4,\n",
        "                  alpha=0.99, epsilon=1e-5, total_timesteps=int(80e6), lrschedule='linear'):\n",
        "       sess = tf_util.get_session()\n",
        "       nenvs = env.num_envs\n",
        "       nbatch = nenvs*nsteps\n",
        "       with tf.variable_scope('a2c_model', reuse=tf.AUTO_REUSE):\n",
        "          # step_model is used for sampling\n",
        "          step_model = policy(nenvs, 1, sess)\n",
        "\n",
        "          # train_model is used to train our network\n",
        "          train_model = policy(nbatch, nsteps, sess)\n",
        "          A = tf.placeholder(train_model.action.dtype, train_model.action.shape)\n",
        "          ADV = tf.placeholder(tf.float32, [nbatch])\n",
        "          R = tf.placeholder(tf.float32, [nbatch])\n",
        "          LR = tf.placeholder(tf.float32, [])\n",
        "\n",
        "          # Calculate the loss\n",
        "          # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss\n",
        "\n",
        "          # Policy loss\n",
        "          neglogpac = train_model.pd.neglogp(A)\n",
        "          # L = A(s,a) * -logpi(a|s)\n",
        "          pg_loss = tf.reduce_mean(ADV * neglogpac)\n",
        "\n",
        "          # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.\n",
        "          entropy = tf.reduce_mean(train_model.pd.entropy())\n",
        "\n",
        "          # Value loss\n",
        "          vf_loss = losses.mean_squared_error(tf.squeeze(train_model.vf), R)\n",
        "\n",
        "          loss = pg_loss - entropy*ent_coef + vf_loss * vf_coef\n",
        "\n",
        "          # Update parameters using loss\n",
        "          # 1. Get the model parameters\n",
        "          params = find_trainable_variables(\"a2c_model\")\n",
        "\n",
        "          # 2. Calculate the gradients\n",
        "          grads = tf.gradients(loss, params)\n",
        "          if max_grad_norm is not None:\n",
        "              # Clip the gradients (normalize)\n",
        "              grads, grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n",
        "          grads = list(zip(grads, params))\n",
        "          # zip aggregate each gradient with parameters associated\n",
        "          # For instance zip(ABCD, xyza) => Ax, By, Cz, Da\n",
        "\n",
        "          # 3. Make op for one policy and value update step of A2C\n",
        "          trainer = tf.train.RMSPropOptimizer(learning_rate=LR, decay=alpha, epsilon=epsilon)\n",
        "\n",
        "          _train = trainer.apply_gradients(grads)\n",
        "\n",
        "          lr = Scheduler(v=lr, nvalues=total_timesteps, schedule=lrschedule)\n",
        "\n",
        "          def train(obs, states, rewards, masks, actions, values):\n",
        "              # Here we calculate advantage A(s,a) = R + yV(s') - V(s)\n",
        "              # rewards = R + yV(s')\n",
        "              advs = rewards - values\n",
        "              for step in range(len(obs)):\n",
        "                  cur_lr = lr.value()\n",
        "\n",
        "              td_map = {train_model.X:obs, A:actions, ADV:advs, R:rewards, LR:cur_lr}\n",
        "              if states is not None:\n",
        "                  td_map[train_model.S] = states\n",
        "                  td_map[train_model.M] = masks\n",
        "              policy_loss, value_loss, policy_entropy, _ = sess.run(\n",
        "                  [pg_loss, vf_loss, entropy, _train],\n",
        "                  td_map\n",
        "              )\n",
        "              return policy_loss, value_loss, policy_entropy\n",
        "\n",
        "\n",
        "          self.train = train\n",
        "          self.train_model = train_model\n",
        "          self.step_model = step_model\n",
        "          self.step = step_model.step\n",
        "          self.value = step_model.value\n",
        "          self.initial_state = step_model.initial_state\n",
        "          self.save = functools.partial(tf_util.save_variables, sess=sess)\n",
        "          self.load = functools.partial(tf_util.load_variables, sess=sess)\n",
        "          tf.global_variables_initializer().run(session=sess)\n",
        "\n",
        "\n",
        "def learn_a2c(\n",
        "    network,\n",
        "    env,\n",
        "    seed=None,\n",
        "    nsteps=5,\n",
        "    total_timesteps=int(80e6),\n",
        "    vf_coef=0.5,\n",
        "    ent_coef=0.01,\n",
        "    max_grad_norm=0.5,\n",
        "    lr=7e-4,\n",
        "    lrschedule='linear',\n",
        "    epsilon=1e-5,\n",
        "    alpha=0.99,\n",
        "    gamma=0.99,\n",
        "    log_interval=100,\n",
        "    load_path=None,\n",
        "    **network_kwargs):\n",
        "  \n",
        "    set_global_seeds(seed)\n",
        "\n",
        "    # Get the nb of env\n",
        "    nenvs = env.num_envs\n",
        "    policy = build_policy(env, network, **network_kwargs)\n",
        "\n",
        "    # Instantiate the model object (that creates step_model and train_model)\n",
        "    model = Model_a2c(policy=policy, env=env, nsteps=nsteps, ent_coef=ent_coef, vf_coef=vf_coef,\n",
        "        max_grad_norm=max_grad_norm, lr=lr, alpha=alpha, epsilon=epsilon, total_timesteps=total_timesteps, lrschedule=lrschedule)\n",
        "    if load_path is not None:\n",
        "        model.load(load_path)\n",
        "\n",
        "    # Instantiate the runner object\n",
        "    runner = Runner(env, model, nsteps=nsteps, gamma=gamma)\n",
        "    epinfobuf = deque(maxlen=100)\n",
        "\n",
        "    # Calculate the batch_size\n",
        "    nbatch = nenvs*nsteps\n",
        "\n",
        "    # Start total timer\n",
        "    tstart = time.time()\n",
        "\n",
        "    for update in range(1, total_timesteps//nbatch+1):\n",
        "        # Get mini batch of experiences\n",
        "        obs, states, rewards, masks, actions, values, epinfos = runner.run()\n",
        "        epinfobuf.extend(epinfos)\n",
        "\n",
        "        policy_loss, value_loss, policy_entropy = model.train(obs, states, rewards, masks, actions, values)\n",
        "        nseconds = time.time()-tstart\n",
        "\n",
        "        # Calculate the fps (frame per second)\n",
        "        fps = int((update*nbatch)/nseconds)\n",
        "        if update % log_interval == 0 or update == 1:\n",
        "            # Calculates if value function is a good predicator of the returns (ev > 1)\n",
        "            # or if it's just worse than predicting nothing (ev =< 0)\n",
        "            ev = explained_variance(values, rewards)\n",
        "            logger.record_tabular(\"nupdates\", update)\n",
        "            logger.record_tabular(\"total_timesteps\", update*nbatch)\n",
        "            logger.record_tabular(\"fps\", fps)\n",
        "            logger.record_tabular(\"policy_entropy\", float(policy_entropy))\n",
        "            logger.record_tabular(\"value_loss\", float(value_loss))\n",
        "            logger.record_tabular(\"explained_variance\", float(ev))\n",
        "            logger.record_tabular(\"eprewmean\", safemean([epinfo['r'] for epinfo in epinfobuf]))\n",
        "            logger.record_tabular(\"eplenmean\", safemean([epinfo['l'] for epinfo in epinfobuf]))\n",
        "            logger.dump_tabular()\n",
        "    return model"
      ],
      "metadata": {
        "id": "2KEK9dFWGyCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actor-Critic Implementation**"
      ],
      "metadata": {
        "id": "BJALFzycG_3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IoT_a2c(X,Y):\n",
        "  env = DummyVecEnv([lambda: bench.Monitor(IoTEnv(data_per_episode=1,dataset=(X, Y)), logger.get_dir())])\n",
        "  network='mlp'\n",
        "  model=learn_a2c(\n",
        "      network,\n",
        "      env,\n",
        "      seed=int(time.time()),\n",
        "      nsteps=5,\n",
        "      total_timesteps=int(1.0e5),\n",
        "      vf_coef=0.5,\n",
        "      ent_coef=0.01,\n",
        "      max_grad_norm=0.5,\n",
        "      lr=1e-3,\n",
        "      lrschedule='linear',\n",
        "      epsilon=1e-5,\n",
        "      alpha=0.99,\n",
        "      gamma=0.99,\n",
        "      log_interval=100)\n",
        "  \n",
        "  return model\n",
        "\n",
        "start_time = time.time()\n",
        "a2c_model1=IoT_a2c(X_train_reshape,Y_train)\n",
        "print(\"A2C Training Time:\", time.time() - start_time)"
      ],
      "metadata": {
        "id": "lqumTa-GHCjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actor-Critic Evaluation: No Attack**"
      ],
      "metadata": {
        "id": "yOekL8IZHwbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IoT_a2c_eval(a2c_model):\n",
        "    attempts, correct, tp, fp, fn, tn, i = 0,0,0,0,0,0,0\n",
        "\n",
        "    env = DummyVecEnv([lambda: IoTEnv(data_per_episode=1, dataset=(X_test_reshape, Y_test), random=False)])\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            obs, done = env.reset(), [False]\n",
        "            while not done[0]:\n",
        "                obs, rew, done, _ = env.step(a2c_model.step(obs[None])[0])\n",
        "\n",
        "                attempts += 1\n",
        "                if rew > 0:\n",
        "                  correct += 1\n",
        "                if Y_test[i]==0 and rew == 1:\n",
        "                    tp += 1\n",
        "                elif Y_test[i]==0 and rew == 0:\n",
        "                    fp += 1\n",
        "                elif Y_test[i]==1 and rew == 0:\n",
        "                    fn += 1\n",
        "                elif Y_test[i]==1 and rew == 1:\n",
        "                    tn += 1\n",
        "                i+=1\n",
        "\n",
        "    except StopIteration:\n",
        "        print()\n",
        "        print('validation done...')\n",
        "        print('Accuracy: {0}%'.format((float(correct) / attempts) * 100))\n",
        "        print('False Positive Rate: {0}%'.format((float(fp) / (fp+tn)) * 100))\n",
        "        print('Confusion Matrix:')\n",
        "        print(tp,fp)\n",
        "        print(fn,tn)\n",
        "        print('Total no.of test values:',attempts)\n",
        "\n",
        "IoT_a2c_eval(a2c_model1)"
      ],
      "metadata": {
        "id": "v7ETkCtoHvcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kronecker-Factored Approximation Model Creation**"
      ],
      "metadata": {
        "id": "vPmXf50nIb9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model_acktr(object):\n",
        "\n",
        "    def __init__(self, policy, ob_space, ac_space, nenvs,total_timesteps, nprocs=32, nsteps=20,\n",
        "                 ent_coef=0.01, vf_coef=0.5, vf_fisher_coef=1.0, lr=0.25, max_grad_norm=0.5,\n",
        "                 kfac_clip=0.001, lrschedule='linear', is_async=True):\n",
        "\n",
        "        self.sess = sess = get_session()\n",
        "        nbatch = nenvs * nsteps\n",
        "        with tf.variable_scope('acktr_model', reuse=tf.AUTO_REUSE):\n",
        "            self.model = step_model = policy(nenvs, 1, sess=sess)\n",
        "            self.model2 = train_model = policy(nenvs*nsteps, nsteps, sess=sess)\n",
        "\n",
        "        A = train_model.pdtype.sample_placeholder([None])\n",
        "        ADV = tf.placeholder(tf.float32, [nbatch])\n",
        "        R = tf.placeholder(tf.float32, [nbatch])\n",
        "        PG_LR = tf.placeholder(tf.float32, [])\n",
        "        VF_LR = tf.placeholder(tf.float32, [])\n",
        "\n",
        "        neglogpac = train_model.pd.neglogp(A)\n",
        "        self.logits = train_model.pi\n",
        "\n",
        "        ##training loss\n",
        "        pg_loss = tf.reduce_mean(ADV*neglogpac)\n",
        "        entropy = tf.reduce_mean(train_model.pd.entropy())\n",
        "        pg_loss = pg_loss - ent_coef * entropy\n",
        "        vf_loss = tf.losses.mean_squared_error(tf.squeeze(train_model.vf), R)\n",
        "        train_loss = pg_loss + vf_coef * vf_loss\n",
        "\n",
        "\n",
        "        ##Fisher loss construction\n",
        "        self.pg_fisher = pg_fisher_loss = -tf.reduce_mean(neglogpac)\n",
        "        sample_net = train_model.vf + tf.random_normal(tf.shape(train_model.vf))\n",
        "        self.vf_fisher = vf_fisher_loss = - vf_fisher_coef*tf.reduce_mean(tf.pow(train_model.vf - tf.stop_gradient(sample_net), 2))\n",
        "        self.joint_fisher = joint_fisher_loss = pg_fisher_loss + vf_fisher_loss\n",
        "\n",
        "        self.params=params = find_trainable_variables(\"acktr_model\")\n",
        "\n",
        "        self.grads_check = grads = tf.gradients(train_loss,params)\n",
        "\n",
        "        with tf.device('/gpu:0'):\n",
        "            self.optim = optim = kfac.KfacOptimizer(learning_rate=PG_LR, clip_kl=kfac_clip,\\\n",
        "                momentum=0.9, kfac_update=1, epsilon=0.01,\\\n",
        "                stats_decay=0.99, is_async=is_async, cold_iter=10, max_grad_norm=max_grad_norm)\n",
        "\n",
        "            # update_stats_op = optim.compute_and_apply_stats(joint_fisher_loss, var_list=params)\n",
        "            optim.compute_and_apply_stats(joint_fisher_loss, var_list=params)\n",
        "            train_op, q_runner = optim.apply_gradients(list(zip(grads,params)))\n",
        "        self.q_runner = q_runner\n",
        "        self.lr = Scheduler(v=lr, nvalues=total_timesteps, schedule=lrschedule)\n",
        "\n",
        "        def train(obs, states, rewards, masks, actions, values):\n",
        "            advs = rewards - values\n",
        "            for step in range(len(obs)):\n",
        "                cur_lr = self.lr.value()\n",
        "\n",
        "            td_map = {train_model.X:obs, A:actions, ADV:advs, R:rewards, PG_LR:cur_lr, VF_LR:cur_lr}\n",
        "            if states is not None:\n",
        "                td_map[train_model.S] = states\n",
        "                td_map[train_model.M] = masks\n",
        "\n",
        "            policy_loss, value_loss, policy_entropy, _ = sess.run(\n",
        "                [pg_loss, vf_loss, entropy, train_op],\n",
        "                td_map\n",
        "            )\n",
        "            return policy_loss, value_loss, policy_entropy\n",
        "\n",
        "\n",
        "        self.train = train\n",
        "        self.save = functools.partial(save_variables, sess=sess)\n",
        "        self.load = functools.partial(load_variables, sess=sess)\n",
        "        self.train_model = train_model\n",
        "        self.step_model = step_model\n",
        "        self.step = step_model.step\n",
        "        self.value = step_model.value\n",
        "        self.initial_state = step_model.initial_state\n",
        "        tf.global_variables_initializer().run(session=sess)\n",
        "\n",
        "def learn_acktr(network, env, seed, total_timesteps=int(40e6), gamma=0.99, log_interval=100, nprocs=32, nsteps=20,\n",
        "                 ent_coef=0.01, vf_coef=0.5, vf_fisher_coef=1.0, lr=0.25, max_grad_norm=0.5,\n",
        "                 kfac_clip=0.001, save_interval=None, lrschedule='linear', load_path=None, is_async=True, **network_kwargs):\n",
        "    set_global_seeds(seed)\n",
        "\n",
        "\n",
        "    if network == 'cnn':\n",
        "        network_kwargs['one_dim_bias'] = True\n",
        "\n",
        "    policy = build_policy(env, network, **network_kwargs)\n",
        "\n",
        "    nenvs = env.num_envs\n",
        "    ob_space = env.observation_space\n",
        "    ac_space = env.action_space\n",
        "    make_model = lambda : Model_acktr(policy, ob_space, ac_space, nenvs, total_timesteps, nprocs=nprocs, nsteps\n",
        "                                =nsteps, ent_coef=ent_coef, vf_coef=vf_coef, vf_fisher_coef=\n",
        "                                vf_fisher_coef, lr=lr, max_grad_norm=max_grad_norm, kfac_clip=kfac_clip,\n",
        "                                lrschedule=lrschedule, is_async=is_async)\n",
        "    if save_interval and logger.get_dir():\n",
        "        import cloudpickle\n",
        "        with open(osp.join(logger.get_dir(), 'make_model.pkl'), 'wb') as fh:\n",
        "            fh.write(cloudpickle.dumps(make_model))\n",
        "    model = make_model()\n",
        "\n",
        "    if load_path is not None:\n",
        "        model.load(load_path)\n",
        "\n",
        "    runner = Runner(env, model, nsteps=nsteps, gamma=gamma)\n",
        "    epinfobuf = deque(maxlen=100)\n",
        "    nbatch = nenvs*nsteps\n",
        "    tstart = time.time()\n",
        "    coord = tf.train.Coordinator()\n",
        "    if is_async:\n",
        "        enqueue_threads = model.q_runner.create_threads(model.sess, coord=coord, start=True)\n",
        "    else:\n",
        "        enqueue_threads = []\n",
        "\n",
        "    for update in range(1, total_timesteps//nbatch+1):\n",
        "        obs, states, rewards, masks, actions, values, epinfos = runner.run()\n",
        "        epinfobuf.extend(epinfos)\n",
        "        policy_loss, value_loss, policy_entropy = model.train(obs, states, rewards, masks, actions, values)\n",
        "        model.old_obs = obs\n",
        "        nseconds = time.time()-tstart\n",
        "        fps = int((update*nbatch)/nseconds)\n",
        "        if update % log_interval == 0 or update == 1:\n",
        "            ev = explained_variance(values, rewards)\n",
        "            logger.record_tabular(\"nupdates\", update)\n",
        "            logger.record_tabular(\"total_timesteps\", update*nbatch)\n",
        "            logger.record_tabular(\"fps\", fps)\n",
        "            logger.record_tabular(\"policy_entropy\", float(policy_entropy))\n",
        "            logger.record_tabular(\"policy_loss\", float(policy_loss))\n",
        "            logger.record_tabular(\"value_loss\", float(value_loss))\n",
        "            logger.record_tabular(\"explained_variance\", float(ev))\n",
        "            logger.record_tabular(\"eprewmean\", safemean([epinfo['r'] for epinfo in epinfobuf]))\n",
        "            logger.record_tabular(\"eplenmean\", safemean([epinfo['l'] for epinfo in epinfobuf]))\n",
        "            logger.dump_tabular()\n",
        "\n",
        "        if save_interval and (update % save_interval == 0 or update == 1) and logger.get_dir():\n",
        "            savepath = osp.join(logger.get_dir(), 'checkpoint%.5i'%update)\n",
        "            print('Saving to', savepath)\n",
        "            model.save(savepath)\n",
        "    coord.request_stop()\n",
        "    coord.join(enqueue_threads)\n",
        "    return model"
      ],
      "metadata": {
        "id": "lR-a-8juIdAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kronecker-Factored Approximation Implementation**"
      ],
      "metadata": {
        "id": "xs-eumLzIgiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IoT_acktr(X,Y):\n",
        "  env = DummyVecEnv([lambda: bench.Monitor(IoTEnv(data_per_episode=1,dataset=(X, Y)), logger.get_dir())])\n",
        "  network='mlp'\n",
        "  seed=int(time.time())\n",
        "  model=learn_acktr(network, env, seed, total_timesteps=int(1.0e5), gamma=0.99, log_interval=100, nprocs=32, nsteps=20,\n",
        "                 ent_coef=0.01, vf_coef=0.5, vf_fisher_coef=1.0, lr=0.25, max_grad_norm=0.5,\n",
        "                 kfac_clip=0.001, save_interval=None, lrschedule='linear', load_path=None, is_async=True)\n",
        "  \n",
        "  return model\n",
        "\n",
        "with tf.variable_scope(\"acktr_model\", reuse=tf.AUTO_REUSE):\n",
        "  start_time = time.time()\n",
        "  acktr_model1=IoT_acktr(X_train_reshape,Y_train)\n",
        "  print(\"ACKTR Training Time:\", time.time() - start_time)"
      ],
      "metadata": {
        "id": "d3tQESVXIi2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kronecker-Factored Approximation Evaluation: No Attack**"
      ],
      "metadata": {
        "id": "jibp-Ku3JJ-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IoT_acktr_eval(acktr_model):\n",
        "    attempts, correct, tp, fp, fn, tn, i = 0,0,0,0,0,0,0\n",
        "\n",
        "    env = DummyVecEnv([lambda: IoTEnv(data_per_episode=1, dataset=(X_test_reshape, Y_test), random=False)])\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            obs, done = env.reset(), [False]\n",
        "            while not done[0]:\n",
        "                obs, rew, done, _ = env.step(acktr_model.step(obs[None])[0])\n",
        "\n",
        "                attempts += 1\n",
        "                if rew > 0:\n",
        "                  correct += 1\n",
        "                if Y_test[i]==0 and rew == 1:\n",
        "                    tp += 1\n",
        "                elif Y_test[i]==0 and rew == 0:\n",
        "                    fp += 1\n",
        "                elif Y_test[i]==1 and rew == 0:\n",
        "                    fn += 1\n",
        "                elif Y_test[i]==1 and rew == 1:\n",
        "                    tn += 1\n",
        "                i+=1\n",
        "\n",
        "    except StopIteration:\n",
        "        print()\n",
        "        print('validation done...')\n",
        "        print('Accuracy: {0}%'.format((float(correct) / attempts) * 100))\n",
        "        print('False Positive Rate: {0}%'.format((float(fp) / (fp+tn)) * 100))\n",
        "        print('Confusion Matrix:')\n",
        "        print(tp,fp)\n",
        "        print(fn,tn)\n",
        "        print('Total no.of test values:',attempts)\n",
        "\n",
        "IoT_acktr_eval(acktr_model1)"
      ],
      "metadata": {
        "id": "rU4g1GU2JLRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label Flipping Attack (LFA)**"
      ],
      "metadata": {
        "id": "iIla8coBuBfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_LFA=X_train.copy()\n",
        "Y_train_LFA=Y_train[:]\n",
        "\n",
        "clusterer = KMeans(n_clusters=2, random_state=10)\n",
        "t0=time.time()\n",
        "cluster_labels = clusterer.fit_predict(X_train_LFA)\n",
        "sample_silhouette_values = silhouette_samples(X_train_LFA, cluster_labels)\n",
        "print(\"sample_silhouette_values=\",sample_silhouette_values)\n",
        "\n",
        "flipped_Y_train=Y_train_LFA\n",
        "counter=0\n",
        "for new_index in range(X_train.shape[0]): \n",
        "    if (sample_silhouette_values[new_index]<0.35):                           #and (flipped_Y_train[new_index]==0)\n",
        "          flipped_Y_train[new_index]=abs(flipped_Y_train[new_index]-1)     #flipped_Y_train[new_index]=1\n",
        "          counter=counter+1\n",
        "\n",
        "print(\"Flipped  counter=\", counter)         \n",
        "t1=time.time()\n",
        "print(\"Time for Label Flipping Attack =\",t1-t0)"
      ],
      "metadata": {
        "id": "VYMsvA9uuHKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Neural Network Evaluation: LFA**"
      ],
      "metadata": {
        "id": "NNmrzbORw-QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DNN_Model(X_train_LFA,flipped_Y_train)"
      ],
      "metadata": {
        "id": "KIDTDHzdw9lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Q-Network Evaluation: LFA**"
      ],
      "metadata": {
        "id": "EEz4eQsvxRRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_LFA_reshape=X_train_LFA.values.reshape(X_train_LFA.shape[0],X_train_LFA.shape[1],1)\n",
        "\n",
        "with tf.variable_scope(\"deepq/eps\", reuse=tf.AUTO_REUSE):\n",
        "  start_time = time.time()\n",
        "  dqn_model2 = IoT_dqn(X_LFA_reshape,flipped_Y_train)\n",
        "  print(\"DQN Training Time:\", time.time() - start_time)\n",
        "\n",
        "IoT_dqn_eval(dqn_model2)"
      ],
      "metadata": {
        "id": "C3KigUeaxUbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proximal Policy Optimization (PPO) Evaluation: LFA**"
      ],
      "metadata": {
        "id": "pdbS1rgWx8tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.variable_scope(\"ppo2_model\", reuse=tf.AUTO_REUSE):\n",
        "  start_time = time.time()\n",
        "  ppo_model2 = IoT_ppo(X_LFA_reshape,flipped_Y_train)\n",
        "  print(\"PPO Training Time:\", time.time() - start_time)\n",
        "\n",
        "IoT_ppo_eval(ppo_model2)"
      ],
      "metadata": {
        "id": "CWXA41zjzOod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actor-Critic Evaluation: LFA**"
      ],
      "metadata": {
        "id": "_R5L3Hm72Oo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "a2c_model2=IoT_a2c(X_LFA_reshape,flipped_Y_train)\n",
        "print(\"A2C Training Time:\", time.time() - start_time)\n",
        "\n",
        "IoT_a2c_eval(a2c_model2)"
      ],
      "metadata": {
        "id": "owOqblI02TyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kronecker-Factored Approximation Evaluation: LFA**"
      ],
      "metadata": {
        "id": "mEcYPrsW2ut7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.variable_scope(\"acktr_model\", reuse=tf.AUTO_REUSE):\n",
        "  start_time = time.time()\n",
        "  acktr_model2=IoT_acktr(X_train_reshape,Y_train)\n",
        "  print(\"ACKTR Training Time:\", time.time() - start_time)\n",
        "\n",
        "IoT_acktr_eval(acktr_model2)"
      ],
      "metadata": {
        "id": "QMvj4r5B21Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label Based Semi-supervised Defense (LSD)**"
      ],
      "metadata": {
        "id": "F5hYDllN2-AC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_LSD=X_train.copy()\n",
        "Y_train_LSD=flipped_Y_train[:]\n",
        "\n",
        "X_val_LSD=X_val.copy()\n",
        "Y_val_LSD=Y_val[:]\n",
        "\n",
        "t4=time.time()\n",
        "\n",
        "labels = np.full(X_train.shape[0], -1)\n",
        "for i in range(X_val.shape[0]):\n",
        "    labels[i] = Y_val_LSD[i]\n",
        "\n",
        "label_spread = LabelSpreading(kernel='knn', alpha=0.85)\n",
        "label_propa=LabelPropagation(kernel='knn', gamma=20, n_neighbors=7, max_iter=1000, tol=0.001, n_jobs=None)\n",
        "label_spread.fit(X_train_LSD, labels)\n",
        "label_propa.fit(X_train_LSD, labels)\n",
        "output_labels_spread = label_spread.transduction_\n",
        "output_labels_propa = label_propa.transduction_"
      ],
      "metadata": {
        "id": "JV65B5ii3ufv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Neural Network (DNN) for LSD**"
      ],
      "metadata": {
        "id": "5BTAJ-lo32wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DNN_model=keras.Sequential(\n",
        "        [\n",
        "            keras.Input(shape=input_shape),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(16, activation='relu'),\n",
        "            layers.Dense(32, activation='relu'),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(num_classes, activation=\"sigmoid\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "DNN_model.summary()\n",
        "\n",
        "DNN_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "DNN_model.fit(X_train_LSD, Y_train_LSD, batch_size=32, epochs=2, validation_split=0.1)\n",
        "\n",
        "Y_predict_DNN_for_LSD=DNN_model.predict(X_train_LSD, verbose=0)\n",
        "\n",
        "Y_predict_DNN_LSD_Final=[0]*len(Y_predict_DNN_for_LSD)\n",
        "for i in range(len(Y_predict_DNN_for_LSD)):\n",
        "    if Y_predict_DNN_for_LSD[i]<0.5:\n",
        "          Y_predict_DNN_LSD_Final[i]=0\n",
        "    else:\n",
        "            Y_predict_DNN_LSD_Final[i]=1\n",
        "\n",
        "print(Y_predict_DNN_LSD_Final)\n",
        "\n",
        "#*******************************************Voting Between DNN , label Propagation and Label Spreading**************************     \n",
        "Y_predict_LSD_Final=[0]*len(Y_train)\n",
        "for i in range(len(Y_train)):\n",
        "    c=Y_train_LSD[i]+Y_predict_DNN_LSD_Final[i]+output_labels_propa[i]+output_labels_spread[i]\n",
        "    if 3<=c:\n",
        "        Y_predict_LSD_Final[i]=1\n",
        "    else:\n",
        "        Y_predict_LSD_Final[i]=0\n",
        "t5=time.time()\n",
        "print(\"Time for Label Based Semi-supervised Defense =\",t5-t4)"
      ],
      "metadata": {
        "id": "2qy2PF2m3-d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Neural Network Evaluation: LSD**"
      ],
      "metadata": {
        "id": "ej_zSQ4K5ERu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DNN_Model(X_train_LSD, np.array(Y_predict_LSD_Final))"
      ],
      "metadata": {
        "id": "jMKuntl05GJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Q-Network Evaluation: LSD**"
      ],
      "metadata": {
        "id": "ZhKfNmyM5NaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_LSD_reshape=X_train_LSD.values.reshape(X_train_LSD.shape[0],X_train_LSD.shape[1],1)\n",
        "\n",
        "with tf.variable_scope(\"deepq/eps\", reuse=tf.AUTO_REUSE):\n",
        "  start_time = time.time()\n",
        "  dqn_model3 = IoT_dqn(X_LSD_reshape,Y_predict_LSD_Final)\n",
        "  print(\"DQN Training Time:\", time.time() - start_time)\n",
        "\n",
        "IoT_dqn_eval(dqn_model3)"
      ],
      "metadata": {
        "id": "Fp8fWYkJ5Pjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proximal Policy Optimization (PPO) Evaluation: LSD**"
      ],
      "metadata": {
        "id": "9HFBDoit5eut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.variable_scope(\"ppo2_model\", reuse=tf.AUTO_REUSE):\n",
        "  start_time = time.time()\n",
        "  ppo_model3 = IoT_ppo(X_LSD_reshape,Y_predict_LSD_Final)\n",
        "  print(\"PPO Training Time:\", time.time() - start_time)\n",
        "\n",
        "IoT_ppo_eval(ppo_model3)"
      ],
      "metadata": {
        "id": "Q7J6FGgG5jxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actor-Critic Evaluation: LSD**"
      ],
      "metadata": {
        "id": "UaTI7dLH5sqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "a2c_model3=IoT_a2c(X_LSD_reshape,Y_predict_LSD_Final)\n",
        "print(\"A2C Training Time:\", time.time() - start_time)\n",
        "\n",
        "IoT_a2c_eval(a2c_model3)"
      ],
      "metadata": {
        "id": "8EXv7F695vF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kronecker-Factored Approximation Evaluation: LSD**"
      ],
      "metadata": {
        "id": "DmbKLOG759DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.variable_scope(\"acktr_model\", reuse=tf.AUTO_REUSE):\n",
        "  start_time = time.time()\n",
        "  acktr_model3=IoT_acktr(X_LSD_reshape,Y_predict_LSD_Final)\n",
        "  print(\"ACKTR Training Time:\", time.time() - start_time)\n",
        "\n",
        "IoT_acktr_eval(acktr_model3)"
      ],
      "metadata": {
        "id": "2OZA4W096BPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clustering Based Semi-supervised Defense (CSD)**"
      ],
      "metadata": {
        "id": "MhCgxluz6Kpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_CSD=X_train.copy()\n",
        "Y_train_CSD=flipped_Y_train[:]\n",
        "\n",
        "X_val_CSD=X_val.copy()\n",
        "Y_val_CSD=Y_val[:]\n",
        "\n",
        "t6=time.time()\n",
        "\n",
        "Y_predict_val_from_DNN_model=DNN_model.predict(X_val_CSD, verbose=0)\n",
        "\n",
        "Y_predict_val_from_DNN_model_Final=[0]*len(Y_predict_val_from_DNN_model)\n",
        "for i in range(len(Y_predict_val_from_DNN_model)):\n",
        "    if Y_predict_val_from_DNN_model[i]<0.5:\n",
        "          Y_predict_val_from_DNN_model[i]=0\n",
        "    else:\n",
        "          Y_predict_val_from_DNN_model[i]=1\n",
        "for i in range(len(Y_predict_val_from_DNN_model)):\n",
        "    Y_predict_val_from_DNN_model_Final[i]= int(Y_predict_val_from_DNN_model[i])\n",
        "    \n",
        "adjusted_rand_score_val=metrics.adjusted_rand_score(Y_val_CSD, Y_predict_val_from_DNN_model_Final)\n",
        "adjusted_mutual_info_score_val=metrics.adjusted_mutual_info_score(Y_val_CSD, Y_predict_val_from_DNN_model_Final) \n",
        "homogeneity_score_val=metrics.homogeneity_score(Y_val_CSD, Y_predict_val_from_DNN_model_Final) \n",
        "fowlkes_mallows_score_val=metrics.fowlkes_mallows_score(Y_val_CSD, Y_predict_val_from_DNN_model_Final) \n",
        "\n",
        "for i in range(20):        #row_train\n",
        "    Y_temp=Y_val_CSD.copy()\n",
        "\n",
        "    X_temp = X_val_CSD\n",
        "    np.append(Y_temp,Y_train_CSD[i])\n",
        "    \n",
        "    Y_predict_CNN_compute_CSD=DNN_model.predict(X_temp, verbose=0)\n",
        "    \n",
        "    Y_predict_temp=[0]*len(Y_predict_DNN_compute_CSD)\n",
        "    \n",
        "    for n in range(len(Y_predict_DNN_compute_CSD)):\n",
        "        if Y_predict_DNN_compute_CSD[n]<0.5:\n",
        "              Y_predict_DNN_compute_CSD[n]=0\n",
        "        else:\n",
        "                Y_predict_DNN_compute_CSD[n]=1\n",
        "      \n",
        "    for m in range(len(Y_predict_DNN_compute_CSD)):\n",
        "        Y_predict_temp[m]= int(Y_predict_DNN_compute_CSD[m])\n",
        "\n",
        "    adjusted_rand_score_temp=metrics.adjusted_rand_score(Y_temp, Y_predict_temp)\n",
        "    adjusted_mutual_info_score_temp=metrics.adjusted_mutual_info_score(Y_temp, Y_predict_temp) \n",
        "    homogeneity_score_temp=metrics.homogeneity_score(Y_temp, Y_predict_temp) \n",
        "    fowlkes_mallows_score_temp=metrics.fowlkes_mallows_score(Y_temp, Y_predict_temp)\n",
        "    \n",
        "    landa1=abs(adjusted_rand_score_temp-adjusted_rand_score_val)\n",
        "    landa2=abs(adjusted_mutual_info_score_temp-adjusted_mutual_info_score_val)\n",
        "    landa3=abs(homogeneity_score_temp-homogeneity_score_val)\n",
        "    landa4=abs(fowlkes_mallows_score_temp-fowlkes_mallows_score_val)\n",
        "    \n",
        "    sum_of_diffrences=landa1+landa2+landa3+landa4\n",
        "    \n",
        "    if sum_of_diffrences<0.1:\n",
        "        X_val_CSD = X_val_CSD\n",
        "        np.append(Y_val_CSD,Y_train_CSD[i])          \n",
        "        Y_predict_DNN_inside_CSD=DNN_model.predict(X_val_CSD, verbose=0)\n",
        "        \n",
        "        Y_predict_DNN_inside_CSD_Final=[0]*len(Y_predict_DNN_inside_CSD)                   #Y_predict_CNN_inside\n",
        "        for j in range(len(Y_predict_DNN_inside_CSD)):                                     #Y_predict_CNN_inside\n",
        "            if Y_predict_DNN_inside_CSD[j]<0.5:\n",
        "                  Y_predict_DNN_inside_CSD[j]=0\n",
        "            else:\n",
        "                  Y_predict_DNN_inside_CSD[j]=1\n",
        "                    \n",
        "        for k in range(len(Y_predict_DNN_inside_CSD)):                              #Y_predict_CNN_inside\n",
        "            Y_predict_DNN_inside_CSD_Final[k]= int(Y_predict_DNN_inside_CSD[k])\n",
        "\n",
        "        adjusted_rand_score_val=metrics.adjusted_rand_score(Y_val_CSD, Y_predict_DNN_inside_CSD_Final)\n",
        "        adjusted_mutual_info_score_val=metrics.adjusted_mutual_info_score(Y_val_CSD, Y_predict_DNN_inside_CSD_Final) \n",
        "        homogeneity_score_val=metrics.homogeneity_score(Y_val_CSD, Y_predict_DNN_inside_CSD_Final) \n",
        "        fowlkes_mallows_score_val=metrics.fowlkes_mallows_score(Y_val_CSD, Y_predict_DNN_inside_CSD_Final) \n",
        "t7=time.time()\n",
        "print(\"Time for Clustering Based Semi-supervised Defense =\",t7-t6)\n",
        "\n",
        "X_train_Final_CSD= X_val_CSD.copy()  \n",
        "Y_train_Final_CSD=Y_val_CSD.copy()"
      ],
      "metadata": {
        "id": "TYjMfG3f6MyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Neural Network Evaluation: CSD**"
      ],
      "metadata": {
        "id": "K1UiGj7D6Vvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DNN_Model(X_train_Final_CSD, Y_train_Final_CSD)"
      ],
      "metadata": {
        "id": "mSmG87Wq6XkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Q-Network Evaluation: CSD**"
      ],
      "metadata": {
        "id": "UuLQ0LqE6cri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_CSD_reshape=X_train_Final_CSD.values.reshape(X_train_Final_CSD.shape[0],X_train_Final_CSD.shape[1],1)\n",
        "\n",
        "with tf.variable_scope(\"deepq/eps\", reuse=tf.AUTO_REUSE):\n",
        "  start_time = time.time()\n",
        "  dqn_model4 = IoT_dqn(X_CSD_reshape, Y_train_Final_CSD)\n",
        "  print(\"DQN Training Time:\", time.time() - start_time)\n",
        "\n",
        "IoT_dqn_eval(dqn_model4)"
      ],
      "metadata": {
        "id": "yfIiHPwR6ewi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proximal Policy Optimization (PPO) Evaluation: CSD**"
      ],
      "metadata": {
        "id": "PgmSXNq76vw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.variable_scope(\"ppo2_model\", reuse=tf.AUTO_REUSE):\n",
        "  start_time = time.time()\n",
        "  ppo_model4 = IoT_ppo(X_CSD_reshape, Y_train_Final_CSD)\n",
        "  print(\"PPO Training Time:\", time.time() - start_time)\n",
        "\n",
        "IoT_ppo_eval(ppo_model4)"
      ],
      "metadata": {
        "id": "OGVpXhil6ycs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actor-Critic Evaluation: CSD**"
      ],
      "metadata": {
        "id": "BLCaS7jn66AF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "a2c_model4=IoT_a2c(X_CSD_reshape, Y_train_Final_CSD)\n",
        "print(\"A2C Training Time:\", time.time() - start_time)\n",
        "\n",
        "IoT_a2c_eval(a2c_model4)"
      ],
      "metadata": {
        "id": "fEHmbILs6-Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kronecker-Factored Approximation Evaluation: CSD**"
      ],
      "metadata": {
        "id": "--STp8sm7ExM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.variable_scope(\"acktr_model\", reuse=tf.AUTO_REUSE):\n",
        "  start_time = time.time()\n",
        "  acktr_model4=IoT_acktr(X_CSD_reshape, Y_train_Final_CSD)\n",
        "  print(\"ACKTR Training Time:\", time.time() - start_time)\n",
        "\n",
        "IoT_acktr_eval(acktr_model4)"
      ],
      "metadata": {
        "id": "DvHDqe5E7Gfj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}